# üî• Pre-trained Transformer Encoder from Scratch (NumPy)

This project is a character-level Transformer Encoder **implemented entirely from scratch using NumPy**, with no deep learning frameworks (no PyTorch, TensorFlow, or Keras). It replicates the core components of the Transformer architecture as introduced in the ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper.

---

## ‚ú® Features

- Character-level tokenization
- Learnable embedding matrix
- Positional encoding (sinusoidal)
- Single-head self-attention
- Feed-forward neural network (FFN)
- Residual connections and layer normalization
---

## üìÅ Project Structure

| File | Description |
|------|-------------|
| `model.py` or notebook | Contains the full model pipeline |
| `README.md` | You're reading it :) |
| `pg48320.txt` |This is actually a Sherlock Holmes Book :)|

---

## üß† What It Does

This project implements a **Transformer Encoder** from scratch in NumPy.

The encoder takes a character-level input sequence and produces a context-aware vector representation for each character. It includes embedding, positional encoding, self-attention, and feed-forward layers ‚Äî mimicking the internal computations of models like BERT and GPT encoders.


